{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9314d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dataloader import Dataloader\n",
    "from agents.qa.test_runner import TestRunner\n",
    "from schemas.tests import TestSuiteBase\n",
    "from modules.ollama import OllamaHandler\n",
    "from agents import *\n",
    "\n",
    "test_runner = TestRunner()\n",
    "\n",
    "data = Dataloader.load_jsonl(\"data/dataset.jsonl\")\n",
    "example = data[\"1873_A\"]\n",
    "test_suite = TestSuiteBase(test_cases=example.private_test_cases)\n",
    "\n",
    "\n",
    "ollama_handler = OllamaHandler(model_name=\"gemma3:latest\")\n",
    "dev = Ellian(ollama_handler)\n",
    "qa = Carlos(ollama_handler)\n",
    "reseacher = Thifany(ollama_handler)\n",
    "judge = Will(ollama_handler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e8cd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddcc5247fb64057a97c8fe1c031a159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Developer generation attempt <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> <span style=\"color: #800080; text-decoration-color: #800080\">/</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mDEBUG   \u001b[0m Developer generation attempt \u001b[1;36m1\u001b[0m \u001b[35m/\u001b[0m \u001b[1;36m3\u001b[0m                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Response generation took <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.27</span> seconds. With <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">175</span> output tokens.                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mDEBUG   \u001b[0m Response generation took \u001b[1;36m4.27\u001b[0m seconds. With \u001b[1;36m175\u001b[0m output tokens.                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">DEBUG   </span> Code parsed successfully.                                                                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mDEBUG   \u001b[0m Code parsed successfully.                                                                                 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> ✅ <span style=\"font-weight: bold\">[</span>END<span style=\"font-weight: bold\">]</span> <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">00:05</span><span style=\"font-weight: bold\">]</span> Parsing generated code                                                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m ✅ \u001b[1m[\u001b[0mEND\u001b[1m]\u001b[0m \u001b[1m[\u001b[0m\u001b[1;92m00:05\u001b[0m\u001b[1m]\u001b[0m Parsing generated code                                                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TestsResult(raw_code='import sys\\n\\ndef solve():\\n    s = input()\\n    if s == \"abc\":\\n        print(\"YES\")\\n        return\\n    \\n    if s == \"acb\":\\n        print(\"YES\")\\n        return\\n    \\n    if s == \"bac\":\\n        print(\"YES\")\\n        return\\n    \\n    if s == \"bca\":\\n        print(\"NO\")\\n        return\\n    \\n    if s == \"cab\":\\n        print(\"NO\")\\n        return\\n    \\n    if s == \"cba\":\\n        print(\"YES\")\\n        return\\n    \\n    print(\"NO\")\\n\\nif __name__ == \"__main__\":\\n    t = int(sys.stdin.readline())\\n    for _ in range(t):\\n        solve()', total_time=0.5505, passed_tests=4, total_tests=4, success_rate=1.0, errors=[])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = dev.generate_code_from_question_dataset(example)\n",
    "results = test_runner.run(test_suite, code)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25533d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "429a2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.orchestrator import Vivi, OrchestratorConfig\n",
    "\n",
    "config = OrchestratorConfig(\n",
    "    model=\"gemma3:latest\",\n",
    "    max_iter=2,\n",
    "    max_retry=3,\n",
    "    dev_verbosity=1,\n",
    "    judge_level=0,\n",
    "    use_buffer=False,\n",
    "    ignore_warnings=True\n",
    ")\n",
    "\n",
    "orchestrator = Vivi(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1cfd5e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "3 validation errors for Solution\nbest_code\n  Field required [type=missing, input_value={'context': \"This approac... to \"abc\", print \"NO\".'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nsuccess_rate\n  Field required [type=missing, input_value={'context': \"This approac... to \"abc\", print \"NO\".'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nsolution_history\n  Field required [type=missing, input_value={'context': \"This approac... to \"abc\", print \"NO\".'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m code_arch = \u001b[43morchestrator\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolve_question_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Faculdade\\TCC\\bench\\agents\\orchestrator.py:64\u001b[39m, in \u001b[36mVivi.solve_question_dataset\u001b[39m\u001b[34m(self, question_dataset)\u001b[39m\n\u001b[32m     62\u001b[39m task = \u001b[38;5;28mself\u001b[39m.researcher.define_task(question_dataset)\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Passo 2 -> Iniciar a resolução da Task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m task = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msolve_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Passo 3 -> Retornar o código final da Task resolvida\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m task.code\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Faculdade\\TCC\\bench\\agents\\orchestrator.py:89\u001b[39m, in \u001b[36mVivi.solve_task\u001b[39m\u001b[34m(self, task, iteration)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Caso a abordagem sugerida seja de subtasks, vamos resolver cada subtask recursivamente\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m ((solve_task_plan.solutions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) ^ (solve_task_plan.subtasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)), \\\n\u001b[32m     86\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEither solutions or subtasks must be provided, not both\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m task = \\\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msolution_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolve_task_plan\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolutions\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolutions\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m solve_task_plan.result_type == \u001b[33m'\u001b[39m\u001b[33msolutions\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m solve_task_plan.solutions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.solve_subtasks(task,solve_task_plan.subtasks,iteration -\u001b[32m1\u001b[39m) \u001b[38;5;66;03m#type: ignore\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Atualiza o template de solução\u001b[39;00m\n\u001b[32m     94\u001b[39m task.template = \u001b[38;5;28mself\u001b[39m.researcher.save_history(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Faculdade\\TCC\\bench\\agents\\orchestrator.py:190\u001b[39m, in \u001b[36mVivi.solution_search\u001b[39m\u001b[34m(self, task, base_solutions)\u001b[39m\n\u001b[32m    188\u001b[39m solutions = []\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m solution \u001b[38;5;129;01min\u001b[39;00m base_solutions:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     solutions.append(\u001b[43mSolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msolution\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# 2. Incializa variáveis de controle\u001b[39;00m\n\u001b[32m    193\u001b[39m best_solution_rate = -\u001b[32m1.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Faculdade\\TCC\\bench\\.venv\\Lib\\site-packages\\pydantic\\main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 3 validation errors for Solution\nbest_code\n  Field required [type=missing, input_value={'context': \"This approac... to \"abc\", print \"NO\".'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nsuccess_rate\n  Field required [type=missing, input_value={'context': \"This approac... to \"abc\", print \"NO\".'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing\nsolution_history\n  Field required [type=missing, input_value={'context': \"This approac... to \"abc\", print \"NO\".'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing"
     ]
    }
   ],
   "source": [
    "code_arch = orchestrator.solve_question_dataset(example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
